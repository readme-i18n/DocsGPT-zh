---

title: 将 DocsGPT 连接到本地推理引擎
description: 将 DocsGPT 连接到本地推理引擎，直接在您的硬件上运行大型语言模型。
---

# 将 DocsGPT 连接到本地推理引擎

DocsGPT 可配置为利用本地推理引擎，使您能够在自有基础设施上直接运行大型语言模型。这种方式能为您提供更强的隐私保护和对 LLM 处理的掌控力。

目前，DocsGPT 主要支持与 OpenAI API 格式兼容的本地推理引擎。这意味着您可以将 DocsGPT 连接到各种模拟 OpenAI API 结构的本地 LLM 服务器。

## 通过 `.env` 文件配置

在 DocsGPT 中设置本地推理引擎需通过 `.env` 文件中的环境变量进行配置。如需了解所有设置的详细说明，请参阅 [DocsGPT 设置指南](/Deploying/DocsGPT-Settings)。

要连接到本地推理引擎，通常需要在 `.env` 文件中配置以下设置：

*   **`LLM_PROVIDER`**: 必须设置为 `openai`。这指示 DocsGPT 使用与 OpenAI 兼容的 API 格式进行通信，即使 LLM 是本地运行的。
*   **`LLM_NAME`**: 指定本地推理引擎识别的模型名称。可以是模型标识符，如果引擎在 API 请求中不需要显式模型命名，则可留为 `None`。
*   **`OPENAI_BASE_URL`**: 这是关键配置。设置为本地推理引擎 API 端点的基准 URL，告知 DocsGPT 如何访问您的本地 LLM 服务器。
*   **`API_KEY`**: 对于本地推理引擎，通常可设置为 `API_KEY=None`，因为本地部署一般不需要身份验证。

## 支持的本地推理引擎 (兼容 OpenAI API)

DocsGPT 可轻松配置以支持以下本地推理引擎，均通过 OpenAI API 格式通信。以下是基于默认配置的各引擎 `OPENAI_BASE_URL` 示例值：

| 推理引擎               | `LLM_PROVIDER` | `OPENAI_BASE_URL`          |
| :---------------------------- | :------------- | :------------------------- |
| LLaMa.cpp                     | `openai`       | `http://localhost:8000/v1`   |
| Ollama                        | `openai`       | `http://localhost:11434/v1`  |
| Text Generation Inference (TGI)| `openai`      | `http://localhost:8080/v1`   |
| SGLang                        | `openai`       | `http://localhost:30000/v1`  |
| vLLM                          | `openai`       | `http://localhost:8000/v1`   |
| Aphrodite                     | `openai`       | `http://localhost:2242/v1`   |
| FriendliAI                    | `openai`       | `http://localhost:8997/v1`   |
| LMDeploy                      | `openai`       | `http://localhost:23333/v1` |

**关于 `localhost` 与 `host.docker.internal` 的重要说明：**

上表中的 `OPENAI_BASE_URL` 示例均使用 `http://localhost`。如果您在 Docker 中运行 DocsGPT 且本地推理引擎运行在宿主机（Docker 外部）上，可能需要将 `localhost` 替换为 `http://host.docker.internal` 以确保 Docker 能正确访问宿主机的服务。例如 Ollama 应使用 `http://host.docker.internal:11434/v1`。

## 添加对其他本地引擎的支持

虽然 DocsGPT 目前主要支持与 OpenAI API 兼容的本地推理引擎，但您可以扩展其功能以支持其他本地推理解决方案。为此，请导航至 DocsGPT 代码仓库中的 `application/llm` 目录。查看现有的 Python 文件以了解 LLM 集成示例。您可以为所需的本地引擎创建新模块，然后在同一目录下的 `llm_creator.py` 文件中进行注册。这样便能实现与上述列表之外的各种本地 LLM 服务器的自定义集成。